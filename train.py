# -*- coding: utf-8 -*-
"""TD3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C0DzbWpB9a1Ci_Pw1kGFHXDMEddB4KsB

# Twin Delayed Deep Deterministic Policy Gradient (TD3)

# Setup
"""

# pip install tensorboardX
# pip install gym
# pip install roboschool

"""# Imports"""

import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F
from tensorboardX import SummaryWriter

import gym
#import roboschool
import sys
import argparse

from components import TD3, ReplayBuffer, Runner

def hidden_init(layer):
    fan_in = layer.weight.data.size()[0]
    lim = 1. / np.sqrt(fan_in)
    return (-lim, lim)

"""# Evaluate"""

def evaluate_policy(policy, env, eval_episodes=100,render=False):
    """run several episodes using the best agent policy
        
        Args:
            policy (agent): agent to evaluate
            env (env): gym environment
            eval_episodes (int): how many test episodes to run
            render (bool): show training
        
        Returns:
            avg_reward (float): average reward over the number of evaluations
    
    """
    
    avg_reward = 0.
    for i in range(eval_episodes):
        obs = env.reset()
        done = False
        while not done:
            if render:
                env.render()
            action = policy.select_action(np.array(obs), noise=0)
            obs, reward, done, _ = env.step(action)
            avg_reward += reward

    avg_reward /= eval_episodes

    print("\n---------------------------------------")
    print("Evaluation over {:d} episodes: {:f}" .format(eval_episodes, avg_reward))
    print("---------------------------------------")
    return avg_reward

"""# Observation"""

def observe(env,replay_buffer, observation_steps):
    """run episodes while taking random actions and filling replay_buffer
    
        Args:
            env (env): gym environment
            replay_buffer(ReplayBuffer): buffer to store experience replay
            observation_steps (int): how many steps to observe for
    
    """
    
    time_steps = 0
    obs = env.reset()
    done = False

    while time_steps < observation_steps:
        action = env.action_space.sample()
        new_obs, reward, done, _ = env.step(action)

        #print("obs: {}, \taction: {}, \tnew_obs: {}, \treward: {}, \tdone: {}".format(obs, action, new_obs, reward, done))
        replay_buffer.add((obs, action, new_obs, reward, done))

        obs = new_obs
        time_steps += 1

        if done:
            obs = env.reset()
            done = False

        print("\rPopulating Buffer {}/{}.".format(time_steps, observation_steps), end="")
        sys.stdout.flush()

"""# Train"""

def train(agent, test_env, replay_buffer, runner):
    """Train the agent for exploration steps
    
        Args:
            agent (Agent): agent to use
            env (environment): gym environment
            writer (SummaryWriter): tensorboard writer
            exploration (int): how many training steps to run
    
    """

    total_timesteps = 0
    timesteps_since_eval = 0
    episode_num = 0
    episode_reward = 0
    episode_timesteps = 0
    reward = None
    done = False 
    obs = test_env.reset()
    evaluations = []
    rewards = []
    best_avg = -8000
    
    writer = SummaryWriter(comment="-TD3_DroneEnv")
    
    while total_timesteps < EXPLORATION:
    
        if done: 

            if total_timesteps != 0: 
                rewards.append(episode_reward)
                avg_reward = np.mean(rewards[-BATCH_SIZE:])
                
                writer.add_scalar("avg_reward", avg_reward, total_timesteps)
                writer.add_scalar("reward_step", reward, total_timesteps)
                writer.add_scalar("episode_reward", episode_reward, total_timesteps)
                
                if best_avg < avg_reward:
                    best_avg = avg_reward
                    print("saving best model....\n")
                    agent.save("best_avg","saves", suffix=args.save_index)

                print("\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}".format(
                    total_timesteps, episode_num, episode_reward, avg_reward), end="")
                sys.stdout.flush()


                if avg_reward >= REWARD_THRESH:
                    break

                agent.train(replay_buffer, episode_timesteps, BATCH_SIZE, GAMMA, TAU, NOISE, NOISE_CLIP, POLICY_FREQUENCY)

                # Evaluate episode
                if timesteps_since_eval >= EVAL_FREQUENCY:
                    timesteps_since_eval %= EVAL_FREQUENCY
                    eval_reward = evaluate_policy(agent, test_env)
                    evaluations.append(avg_reward)
                    writer.add_scalar("eval_reward", eval_reward, total_timesteps)

                    if best_avg < eval_reward:
                        best_avg = eval_reward
                        print("saving best model....\n")
                        agent.save("best_avg","saves", suffix=args.save_index)

                episode_reward = 0
                episode_timesteps = 0
                episode_num += 1 

        reward, done = runner.next_step(episode_timesteps)
        episode_reward += reward

        episode_timesteps += 1
        total_timesteps += 1
        timesteps_since_eval += 1

"""# Config"""

ENV = "gym_drone:drone-v0" #"Pendulum-v0" #"RoboschoolHalfCheetah-v1"
SEED = 0
OBSERVATION = 10000
EXPLORATION = 5000000
BATCH_SIZE = 100
GAMMA = 0.99
TAU = 0.005
NOISE = 0.2
NOISE_CLIP = 0.5
EXPLORE_NOISE = 0.1
POLICY_FREQUENCY = 2
EVAL_FREQUENCY = 10000
REWARD_THRESH = 1000

"""# Main"""
def main(args):
    env = gym.make(ENV)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Set seeds
    env.seed(SEED)
    torch.manual_seed(SEED)
    np.random.seed(SEED)

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0] 
    min_action = float(env.action_space.low[0])
    max_action = float(env.action_space.high[0])

    print("Initializing...")
    print("State dim: {}, Action dim: {}, Min action: {}, Max action: {}".format(state_dim, action_dim, min_action, max_action))

    policy = TD3(state_dim, action_dim, (min_action, max_action), env, device)

    replay_buffer = ReplayBuffer()

    runner = Runner(env, policy, replay_buffer)

    total_timesteps = 0
    timesteps_since_eval = 0
    episode_num = 0
    done = True

    # Populate replay buffer
    observe(env, replay_buffer, OBSERVATION)

    # Train agent
    train(policy, env, replay_buffer, runner)

    policy.load(suffix=args.save_index)

    for i in range(100):
        evaluate_policy(policy, env, render=True)

    env.close()

def parse_args():
    parser = argparse.ArgumentParser(description="TD3 Drone Flight Trainer")
    parser.add_argument("--save_index", help="Allows for multiple simultaneous runs ('-1' e.g.)", default="")
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    main(args)

